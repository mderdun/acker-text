{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the McBride and Wark biographies of Acker to construct a detailed composite biography of Acker's life up to the publication of Black Tarantula. Using this fabricated composite biography, we can distill it into data points based around distinct \"events\" in Acker's life, with a description of the event as well as mention of all actors involved, and finally which sentences from the original biographies were used to inform and define an event.\n",
    "\n",
    "This can be visualised as a network exploration in itself, but it can also be compared against extracts from Black Tarantula to identify biographical influences on the texts, in the same way more apparent literary sources are identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Create a dictionary to store the chapters for each XML file\n",
    "chapters_dict = {}\n",
    "\n",
    "# Parse XML\n",
    "tree = ET.parse('xml/bio/mcbride.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Iterate over all divs with type chapter\n",
    "for div in root.findall('.//{http://www.tei-c.org/ns/1.0}div[@type=\"chapter\"]'):\n",
    "    string = \"\"\n",
    "    result = []\n",
    "    # Iterate over all paragraphs in the respective chapter\n",
    "    for p in div.findall('.//{http://www.tei-c.org/ns/1.0}p'):\n",
    "        # Removing all empty nodes\n",
    "        if p.text is None:\n",
    "            p.text = ''\n",
    "        # Incorporate the text contents of the <quote> elements into the <p> text\n",
    "        for quote in p.findall('.//{http://www.tei-c.org/ns/1.0}quote'):\n",
    "            if quote.text is not None:\n",
    "                p.text += quote.text\n",
    "            for lg in quote.findall('.//{http://www.tei-c.org/ns/1.0}lg'):\n",
    "                if lg.text is not None:\n",
    "                    p.text += lg.text\n",
    "                for l in lg.findall('.//{http://www.tei-c.org/ns/1.0}l'):\n",
    "                    if l.text is not None:\n",
    "                        p.text += l.text\n",
    "        string += str(p.text) + \" \"\n",
    "    result = \"\".join(string)\n",
    "\n",
    "    # Add the chapter with only its respective text to the dictionary\n",
    "    chapters_dict[div.attrib['n']] = result\n",
    "\n",
    "# Write the dictionary to a JSON file and save to the xml folder\n",
    "with open('xml/bio/mcbride_chapters.json', 'w') as fp:\n",
    "    json.dump(chapters_dict, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.05k/1.05k [00:00<00:00, 1.13MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 990M/990M [02:05<00:00, 7.91MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.36k/2.36k [00:00<00:00, 6.99MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 4.56MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 4.55MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 9.10MB/s]\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [2717], which does not match the required output shape [1, 2717]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 22, 128, 1], which does not match the required output shape [1, 22, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/modeling_utils.py:859: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [2885], which does not match the required output shape [1, 2885]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 23, 128, 1], which does not match the required output shape [1, 23, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [2466], which does not match the required output shape [1, 2466]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 20, 128, 1], which does not match the required output shape [1, 20, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [2716], which does not match the required output shape [1, 2716]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [5275], which does not match the required output shape [1, 5275]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 42, 128, 1], which does not match the required output shape [1, 42, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [1831], which does not match the required output shape [1, 1831]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 15, 128, 1], which does not match the required output shape [1, 15, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [5433], which does not match the required output shape [1, 5433]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 43, 128, 1], which does not match the required output shape [1, 43, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [3803], which does not match the required output shape [1, 3803]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 30, 128, 1], which does not match the required output shape [1, 30, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [2878], which does not match the required output shape [1, 2878]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [5885], which does not match the required output shape [1, 5885]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 46, 128, 1], which does not match the required output shape [1, 46, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [4332], which does not match the required output shape [1, 4332]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 34, 128, 1], which does not match the required output shape [1, 34, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [5897], which does not match the required output shape [1, 5897]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 47, 128, 1], which does not match the required output shape [1, 47, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [5235], which does not match the required output shape [1, 5235]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 41, 128, 1], which does not match the required output shape [1, 41, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [6948], which does not match the required output shape [1, 6948]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 55, 128, 1], which does not match the required output shape [1, 55, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [7561], which does not match the required output shape [1, 7561]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 60, 128, 1], which does not match the required output shape [1, 60, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [5868], which does not match the required output shape [1, 5868]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [4546], which does not match the required output shape [1, 4546]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 36, 128, 1], which does not match the required output shape [1, 36, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:171: UserWarning: An output with one or more elements was resized since it had shape [2984], which does not match the required output shape [1, 2984]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/Users/mderdun/anaconda3/envs/ackerText/lib/python3.11/site-packages/transformers/models/longt5/modeling_longt5.py:147: UserWarning: An output with one or more elements was resized since it had shape [1, 24, 128, 1], which does not match the required output shape [1, 24, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:35.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    \"pszemraj/long-t5-tglobal-base-16384-book-summary\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "# Load the JSON file with the chapter texts\n",
    "with open('xml/bio/mcbride_chapters.json', 'r') as fp:\n",
    "    chapters_dict = json.load(fp)\n",
    "\n",
    "# Iterate through the chapter texts and summarize each one\n",
    "for chapter, text in chapters_dict.items():\n",
    "    result = summarizer(text)\n",
    "    summary = result[0][\"summary_text\"]\n",
    "    # Add the summary to the dictionary with the corresponding chapter\n",
    "    chapters_dict[chapter] = {\"text\": text, \"summary\": summary}\n",
    "\n",
    "# Write the updated dictionary to the JSON file\n",
    "with open('xml/bio/mcbride_chapters.json', 'w') as fp:\n",
    "    json.dump(chapters_dict, fp, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ackerText",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
